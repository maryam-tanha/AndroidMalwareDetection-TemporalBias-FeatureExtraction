from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score,roc_auc_score, confusion_matrix, precision_recall_curve
from sklearn.model_selection import GridSearchCV
import pandas as pd
import numpy as np
import sys
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from operator import itemgetter

import shap

import matplotlib.pyplot as plt
import datetime

import dice_ml
from dice_ml.utils import helpers  # helper functions

from sklearn.inspection import permutation_importance

def redirect_stdout_to_file(file_path):
    sys.stdout = open(file_path, 'w')
    
def load_combined_dataset(file_path):
	df = pd.read_csv(file_path)
	return df


def prepare_data_for_classification_imbalanced(df, target_column):

	
	X = df.drop(target_column, axis=1)

	y = df[target_column]
	
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)

	# We want to store the SHA256 values for the test set set only
	#y_test = y_test.values
	#y_train=y_train.values
	
	
	test_sha256_list = X_test['sha256'].values
	
	
	
	X_test.to_csv('test_dataset_imbalanced_'+ timestamp +'.csv', index=False)
	
	
	X_test = X_test.drop(columns=['sha256'])
	temp_train = X_train.drop(columns=['sha256'])
	
	temp_train = pd.concat([temp_train, y_train], axis=1)
	X_train = X_train.drop(columns=['sha256'])
	

	return X_train, y_train, X_test, y_test, test_sha256_list, temp_train
	
	
def prepare_data_for_classification_balanced(training_data, test_data):


    train = pd.DataFrame(training_data)
    test = pd.DataFrame(test_data)
    # Randomly shuffling the rows of the dataframes
    train = train.sample(frac=1, random_state=42)  # Setting a random seed for reproducibility
    test = test.sample(frac=1, random_state=42)  # Setting a random seed for reproducibility
    train.reset_index(drop=True, inplace=True)
    test.reset_index(drop=True, inplace=True)

    # Calculating the ratio of benign to malware APKs in training set
    malware_column_train = train['Malware']
    ratio_ones_to_zeroes = malware_column_train.value_counts(normalize=True)
    print("TRAINING SET: Ratio of Benign APKs to Malware APKs:")
    print(ratio_ones_to_zeroes)

    # Calculating the ratio of benign to malware APKs
    malware_column_test = test['Malware']
    ratio_ones_to_zeroes = malware_column_test.value_counts(normalize=True)
    print("TEST SET: Ratio of Benign APKs to Malware APKs:")
    print(ratio_ones_to_zeroes)

    
    test_sha256_list = test['sha256']
    
    # Set display options
    pd.set_option('display.max_rows', None)
    pd.set_option('display.max_columns', None)
    train = train.drop(['sha256'], axis=1)
    test = test.drop(['sha256'], axis=1)

    # Print the names of all columns
    column_names = train.columns
    column_names_list = column_names.to_list()

    for col in column_names_list:
        train[col] = train[col].astype(int)
        test[col] = test[col].astype(int)

    d_train = train.copy() #train without sha256 col
    
    # Split data into features and target
  
    X_train = train.drop(columns=['Malware'])
    y_train = train['Malware']

    # Split data into features and target
    X_test = test.drop(columns=['Malware'])
    y_test = test['Malware']
    
       
    return X_train, y_train, X_test, y_test, test_sha256_list, d_train



def build_and_evaluate_model():

	print("\n========================================================== Random Forest =======================================================================:")
	model_name = "RF"
	# Parameter Grid for Random Forest
	param_grid_rf = {
		'n_estimators': [50, 100, 200, 500, 1000],
		'max_depth': [None, 5, 10, 16, 20, 30, 40],
		'min_samples_split': [2, 5, 10, 20, 50]
	}
	
	
	if is_balanced:		
		rf_model = RandomForestClassifier()
	else:
		rf_model = RandomForestClassifier(class_weight='balanced')
		

	grid_search_rf = GridSearchCV(rf_model, param_grid_rf, cv=5, scoring='f1', verbose=2, n_jobs=-1)

	# Fit the models
	grid_search_rf.fit(X_train, y_train)


	# Predict and evaluate using the best model
	best_rf = grid_search_rf.best_estimator_
	print("Best Parameters: ", grid_search_rf.best_params_)
	
	compute_model_results(model_name, best_rf)
	
	
	
	#print("\n=========================================================== XGBoost =======================================================================:")
	#model_name = "XGB"
	
	# Define the parameter grid for XGBoost
	#param_grid_xgb = {
    #    'n_estimators': [50, 100, 200, 500, 1000],
    #    'max_depth': [3, 6, 9, 12],
    #    'learning_rate': [0.001, 0.01, 0.1, 0.3],
     #   'subsample': [0.5, 0.8, 1.0],
     #   'colsample_bytree': [0.5, 0.8, 1.0]
    #}

	
	#if is_balanced:	
	#	xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
	#else:
	#	ratio = 9 # 9000/1000 , we have 9000 benign apps (negative class) and 1000 malware
		
		#xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=ratio)
	

	
	#grid_search_xgb = GridSearchCV(xgb_model, param_grid_xgb, cv=5, scoring='f1', verbose=2, n_jobs=-1)
	
	#grid_search_xgb.fit(X_train, y_train)
	
	#best_xgb = grid_search_xgb.best_estimator_
	#print("Best Parameters for XGBoost: ", grid_search_xgb.best_params_)
	
	#compute_model_results(model_name, best_xgb)
	
	return 


def compute_model_results(mod_name, ml_model):

	print("-------------------Results-------------------------:")
	test_predictions = ml_model.predict(X_test)
	train_predictions = ml_model.predict(X_train)


	train_accuracy = accuracy_score(y_train, train_predictions)
	test_accuracy = accuracy_score(y_test, test_predictions)

	train_precision = precision_score(y_train, train_predictions)
	test_precision = precision_score(y_test, test_predictions)

	train_recall = recall_score(y_train, train_predictions)
	test_recall = recall_score(y_test, test_predictions)

	train_f1 = f1_score(y_train, train_predictions)
	test_f1 = f1_score(y_test, test_predictions)

	train_probs = ml_model.predict_proba(X_train)[:, 1]  
	test_probs = ml_model.predict_proba(X_test)[:, 1]

	train_roc_auc = roc_auc_score(y_train, train_probs)
	test_roc_auc = roc_auc_score(y_test, test_probs)

	train_conf_matrix = confusion_matrix(y_train, train_predictions)
	test_conf_matrix = confusion_matrix(y_test, test_predictions)
	#The order in output
	#[[TN, FP],
	#[FN, TP]]
	TN, FP, FN, TP = test_conf_matrix.ravel()

	print(f"Training Accuracy: {train_accuracy}, Test Accuracy: {test_accuracy}")
	print(f"Training Precision: {train_precision}, Test Precision: {test_precision}")
	print(f"Training Recall: {train_recall}, Test Recall: {test_recall}")
	print(f"Training F1 Score: {train_f1}, Test F1 Score: {test_f1}")
	print(f"Training ROC-AUC: {train_roc_auc}, Test ROC-AUC: {test_roc_auc}")
	print(f"Training Confusion Matrix:\n{train_conf_matrix}")
	print(f"Test Confusion Matrix:\n{test_conf_matrix}")
	print("For test dataset:")
	print(f"True Negatives (TN): {TN}")
	print(f"False Positives (FP): {FP}")
	print(f"False Negatives (FN): {FN}")
	print(f"True Positives (TP): {TP}")

	#------------------------------------
	print(classification_report(y_test, test_predictions, labels=[0, 1], target_names=["Benign", "Malware"]))
	
	# Create a test results DataFrame

	test_results = pd.DataFrame({'SHA256': test_sha256, 'Actual Malware': y_test, 'Predicted label':test_predictions})

	#Extract the SHA256 hashes for FPs, FNs, TPs, TNs
	
	fp_hashes = []
	fn_hashes = []
	tn_hashes = []
	tp_hashes = []
	
	for index, row in test_results.iterrows():
		if row['Actual Malware'] == 0 and row['Predicted label'] == 1:
			fp_hashes.append(row['SHA256'])
		elif row['Actual Malware'] == 1 and row['Predicted label'] == 0:	
			fn_hashes.append(row['SHA256'])
		elif row['Actual Malware'] == 0 and row['Predicted label'] == 0:
			tn_hashes.append(row['SHA256'])
		elif row['Actual Malware'] == 1 and row['Predicted label'] == 1:
			tp_hashes.append(row['SHA256'])

	print(f"SHA256 hashes for False Positives (FP): {fp_hashes}")
	print(f"Number of False Positives (FP): {len(fp_hashes)}")
		
	print(f"SHA256 hashes for False Negatives (FN): {fn_hashes}")
	print(f"Number of False Negatives (FN): {len(fn_hashes)}")
	
	print(f"SHA256 hashes for True Negatives (TN): {tn_hashes}")
	print(f"Number of True Negatives (TN): {len(tn_hashes)}")
	
	print(f"SHA256 hashes for True Positives (TP): {tp_hashes}")
	print(f"Number of True Positives (TP): {len(tp_hashes)}")
	
	test_results = pd.concat([test_results, X_test], axis=1)
	
	if is_balanced:
		test_results.to_csv('test_dataset_predictions_balanced_'+mod_name+'_'+timestamp+'.csv', index=False)
	else:
		test_results.to_csv('test_dataset_predictions_imbalanced_'+mod_name+'_'+timestamp+'.csv', index=False)
		
		
		
	#fp_indices = [i for i in range(len(y_test)) if y_test[i] == 0 and test_predictions[i] == 1]
	#fn_indices = [i for i in range(len(y_test)) if y_test[i] == 1 and test_predictions[i] == 0]
	#tn_indices = [i for i in range(len(y_test)) if y_test[i] == 0 and test_predictions[i] == 0]
	#tp_indices = [i for i in range(len(y_test)) if y_test[i] == 1 and test_predictions[i] == 1]
	
	fp_indices = [i for i in range(len(y_test)) if y_test.iloc[i] == 0 and test_predictions[i] == 1]
	fn_indices = [i for i in range(len(y_test)) if y_test.iloc[i] == 1 and test_predictions[i] == 0]
	tn_indices = [i for i in range(len(y_test)) if y_test.iloc[i] == 0 and test_predictions[i] == 0]
	tp_indices = [i for i in range(len(y_test)) if y_test.iloc[i] == 1 and test_predictions[i] == 1]

	print(f'Number of FPs is {len(fp_indices)}.')
	print(f'Number of FNs is {len(fn_indices)}.')
	print(f'Number of TPs is {len(tp_indices)}.')
	print(f'Number of TNs is {len(tn_indices)}.')
	
	#false_negative_indices_new  = []  #stores a subset of FNs in which atleast one feature is equal to one
	
	#for idx in range(len(fn_indices)):
		# Choose one of the false negatives (e.g., the first one)
		
		#chosen_index = fn_indices[idx]

		# Get the feature values for the chosen false negative instance
		#chosen_instance_features = X_test.iloc[chosen_index]

		
		#for column_name, value in chosen_instance_features.items():
			#if value == 1:
				#print(column_name)
				#if chosen_index not in false_negative_indices_new:
					#false_negative_indices_new.append(chosen_index)
					#break
					

	# Print the indexes of false negatives
	#print(f"Filtered FN indices: {false_negative_indices_new }")
	#print(f"Number of filtered FNs: {len(false_negative_indices_new) }")
	
	#true_positive_indices_new  = []  #stores a subset of FPs in which atleast one feature is equal to one
	
	
	#for idx in range(len(tp_indices)):
		# Choose one of the true positives (e.g., the first one)
		
		#chosen_index = tp_indices[idx]

		# Get the feature values for the chosen true positive instance
		#chosen_instance_features = X_test.iloc[chosen_index]

		
		#for column_name, value in chosen_instance_features.items():
			#if value == 1:
				#print(column_name)
				#if chosen_index not in true_positive_indices_new:
					#true_positive_indices_new.append(chosen_index)
					#break
					

	# Print the indexes of TPs
	#print(f"Filtered TP indices: {true_positive_indices_new}")
	#print(f"Number of filtered TPs: {len(true_positive_indices_new) }")
	
	all_runs_top_features_cf = [] # for calculating stability, will be  a list of lists
	all_runs_top_features_shap = []
	num_runs = 10
	for n in range(num_runs):
		print("------------------------------------------------------------------------------------------------------")
		print(f"Run {n+1}:")
		print("------------------------------------------------------------------------------------------------------")
		run_index = n + 1
		top_global_features = create_counterfactuals(ml_model, df_train,fn_indices, tp_indices, run_index)
		
		all_runs_top_features_cf.append(top_global_features)
		
		top_features_shap = compute_shap_values(ml_model, fn_indices, tp_indices)
		count,common_features = find_common_elements(top_features_shap, top_global_features)
		all_runs_top_features_shap.append(top_features_shap)
		print("================================================================")
		print(f"\n Number of common top features between CF and SHAP is {count}")
		print(f"\n Common features are {common_features}")
		print("================================================================")
		
	
	st_cf = compute_stability(all_runs_top_features_cf)
	print("===================================================")
	print(f"\n Average stability for CFs in 10 runs is {st_cf}")
	print("===================================================")
	
	st_shap = compute_stability(all_runs_top_features_shap)
	print("==========================================================")
	print(f"\n Average stability for SHAP in 10 runs is {st_shap}")
	print("==========================================================")
	
	
	return

def compute_stability(features_list):
	print("================================================")
	print("\n Computing stability explanations ...")
	print("================================================")
	k_top_features = 10
	stab = 0
	total = 0
	for	i in range(len(features_list)):
			for	j in range(i+1, len(features_list)):
				counter, common = find_common_elements(features_list[i], features_list[j])
				ratio = counter/k_top_features
				print(f"Explanation stability for run {i+1} and run {j+1} is {ratio}")
				total = total + 1
				stab = stab + ratio
				
	avg_stab = stab/total			
	return avg_stab			
				
		
def compute_shap_values(ml_model,false_negatives_idx, true_positives_idx):
	print("==================================")
	print("\n Computing Shap Values ...")
	print("==================================")
	
	
	#shap.initjs()
	
	tp_instances = X_test[true_positives_idx[0]:true_positives_idx[0]+1]  # a dataframe with the first TP as a row
	true_positives_idx.remove(true_positives_idx[0])
	for idx in true_positives_idx:
		new_row = X_test[idx:idx+1]
		tp_instances = tp_instances.append(new_row, ignore_index=True)
		
	
	explainer = shap.Explainer(ml_model)
	#shap_values = explainer(X_test)
	shap_values = explainer(tp_instances)
	
	feature_names = shap_values.feature_names
	
	shap_sum = np.abs(shap_values.values).mean(axis=0)
	importance_df = pd.DataFrame([feature_names, shap_sum.tolist()]).T
	importance_df.columns = ['Feature', 'SHAP Importance']
	importance_df = importance_df.sort_values('SHAP Importance', ascending=False)
	print(importance_df)
	selected = importance_df.head(10) 
	
	top_10_features= selected['Feature'].values
	
	#top_10_features = importance_df.loc[:9, 'Feature']
	print(f'Top 10 features using SHAP: {top_10_features}')
	
	#shap.summary_plot(shap_values, tp_instances, max_display=10) # i.e., for class 1,i.e., malware
	#shap_values_positive_class = shap_values[1]
	#print(shap_values_positive_class)

	return top_10_features

	
	
	
	#shap_interaction_values_auto = shap_explainer(X_test.iloc[:num_samples], interactions = True).values
	#print(shap_interaction_values_auto.shape)
	#print(shap_interaction_values_auto)
	

	#return   

def compute_permutation_importance(ml_model):
	print("--------------------Permutation importance ------------------------")
	# Calculate recall as the baseline metric
	#baseline_recall = recall_score(y_test, test_predictions)
	#print(f"Baseline Recall: {baseline_recall:.2f}")

	# Calculate permutation importance
	perm_importance = permutation_importance(ml_model, X_test, y_test, scoring='recall', n_repeats=30,n_jobs=-1)
	#perm_importance = permutation_importance(ml_model, X_test, y_test, scoring='recall', n_repeats=30, random_state=42,n_jobs=-1)

	# Get feature importances and sort them
	feature_importances = perm_importance.importances_mean
	for i, v in enumerate(feature_importances):
		if v> 0:
			print(f'Feature: {X_test.columns[i]}, Score: {v}')
	# Sort the dictionary based on values (descending order)

	#sorted_importance_descending = dict(sorted(feature_importances.items(), key=lambda item: item[1], reverse=True))
	#print(sorted_dict_descending)
	#for i, v in enumerate(sorted_importance_descending):
		#if v > 0:
			#print(f'Feature: {X_test.columns[i]}, Score: {v}')

	temp_dict ={}
	print(type(feature_importances))
	for i, v in enumerate(feature_importances):
		if v > 0:
			#print(f'Feature: {X_test.columns[i]}, Score: {v}')
			temp_dict[X_test.columns[i]] = v
	sorted_importance_descending = sorted(temp_dict.items(), key=lambda item: item[1], reverse=True)
	#print(sorted_importance_descending)
	k = 10
	top_k_items = sorted_importance_descending[:k]

	print("--------------------------------------------")
	print("Top 10 features, permutation importance")
	print("--------------------------------------------")
	#top_k = list(top_k_items.keys())
	print(top_k_items)
	
	top_10_list = []
	for item, value in top_k_items:
		#print(f'{item}: {value}')
		top_10_list.append(item) 
	
	print(top_10_list)
	print("-------------------------------------------------------------")
	return top_10_list
def create_counterfactuals(ml_model,train_df, false_negatives_idx, true_positives_idx, run_idx):
	print("==================================")
	print("\n Creating Counterfactuals....")
	print("==================================")
	# Setup DiCE
	d_train = dice_ml.Data(dataframe=train_df, continuous_features=[], outcome_name='Malware')


	dice_model=  dice_ml.Model(model=ml_model, backend='sklearn')

	explainer = dice_ml.Dice(d_train, dice_model, method="random")
	
	
	# Setting parameters for counterfactual generation
	cf_parameters = {
		"total_CFs": 2,  # Number of counterfactuals to generate
		"desired_class": "opposite",  # Asking for the opposite class
		"proximity_weight": 1.5,  # Weight for proximity loss, means how similar the counterfactuals are to the original instance, higher value means closer to the original instance
		"diversity_weight": 1.0  # Weight for diversity , higher value means more varied CFs
		#"sparsity_weight":1.0 # higher ange means less changes in features
		
	}

	if run_idx ==1:#only create CFs for FNs and TPs in the first run
		print("--------------------------------------")
		print("Generating Counterfactuals for FNs....")
		print("--------------------------------------")
		print(f'CF parametrs: {cf_parameters}')		
		#random_fn = random.choice(false_negatives_idx)
		train_df= train_df.drop(['Malware'], axis=1)
		

		for fn_idx in false_negatives_idx:
			row=X_test.iloc[fn_idx]
			print(f'Malware: {y_test.iloc[fn_idx]}')
			#for column_name, value in row.items():
				#if value == 1:
					#print(f"Column Name: {column_name}")
							
			# Sample query instance from the test set
			query_instance = pd.DataFrame(X_test.iloc[fn_idx]).T
			not_found = False
			
			for p_weight in [1.5, 1, 0.9, 0.8, 0.7, 0.6, 0.5]:
				cf_parameters['proximity_weight'] = p_weight 
				try:
					
					# Generate counterfactuals
					counterfactuals = explainer.generate_counterfactuals(query_instance, **cf_parameters)
					print("Success: Found a CF!")
					break #successful in finding a CF
				except Exception as e:
					print(f"Configuration Error: {e} for proximity_weight = {p_weight}")
					print(f'fn_idx: {fn_idx}')
					if  p_weight == 0.5:
						not_found = True
				
					continue
			
			if not_found:
				continue #continue to the next FN
			
					
			# Get the original instance as a Pandas DataFrame
			original_instance = pd.DataFrame(query_instance, columns=train_df.columns)
			print(f"Proximity_weight = {cf_parameters['proximity_weight']}")
			print(f'fn_idx: {fn_idx}')

			print("Counterfactual Explanations:")
			#Iterate through the counterfactuals
			for i, cf in enumerate(explainer.final_cfs):
				print("--------------------------------------------")
				print(f"Counterfactual {i+1} (Changed Features):")
				print("--------------------------------------------")
				
				# Iterate through the features and their values in the counterfactual
				for feature, cf_value, original_value in zip(train_df.columns, cf, original_instance.iloc[0]):
					if cf_value != original_value:
						print(f"{feature}: Original={original_value}, Counterfactual={cf_value}")

		print("--------------------------------------")
		print("Counterfactuals for TPs....")
		print("--------------------------------------")
		for tp_idx in true_positives_idx:
			
			row=X_test.iloc[tp_idx]
			#print(f'Malware: {y_test.iloc[tp_idx]}')
			#for column_name, value in row.items():
				#if value == 1:
					#print(f"Column Name: {column_name}")
							
			# Sample query instance from the test set
			query_instance = pd.DataFrame(X_test.iloc[tp_idx]).T
			
			print("--------------------------------------------------------------")
			print(f"Counterfactuals and local importance for tp_idx: {tp_idx}....")
			print("--------------------------------------------------------------")
			
			try:
				counterfactuals = explainer.generate_counterfactuals(query_instance, total_CFs=10, desired_range=None, desired_class="opposite",permitted_range=None, features_to_vary="all")
				#counterfactuals.visualize_as_dataframe(show_only_changes=True)
				imp = explainer.local_feature_importance(query_instance, cf_examples_list=counterfactuals.cf_examples_list)
				local_feature_imporatnces = imp.local_importance
				temp_dict = local_feature_imporatnces[0]		
				
				none_zero_features = {key:val for key, val in temp_dict.items() if val!=0}
				print(none_zero_features)
				top_10_features = dict(sorted(none_zero_features.items(), key=itemgetter(1), reverse=True)[:10])
				
				#top_10_features = sorted(none_zero_features, key=none_zero_features.get, reverse=True)[:10]
				print("--------------------------------------------")
				print("Top 10 features considering local importance")
				print("--------------------------------------------")
				print(list(top_10_features.keys()))
				

			except Exception as e:
					print(f"Configuration Error: {e}")
					print(f'tp_idx: {tp_idx}')		
					continue
				
		
	
	print("===============================================")
	print("Computing global feature importance for TPs....")
	print("===============================================")
	
	
	query_instances = X_test[true_positives_idx[0]:true_positives_idx[0]+1]  # a dataframe with the first TP as a row
	true_positives_idx.remove(true_positives_idx[0])
	for idx in true_positives_idx:
		new_row = X_test[idx:idx+1]
		query_instances = query_instances.append(new_row, ignore_index=True)
	#print(f'The type of query_instances is {type(query_instances)}.')	
	imp = explainer.global_feature_importance(query_instances)
	global_feature_imporatnces = imp.summary_importance
	
	none_zero_features_global = {key:val for key, val in global_feature_imporatnces.items() if val!=0}
	print(none_zero_features_global)
	top_10_features = dict(sorted(none_zero_features_global.items(), key=itemgetter(1), reverse=True)[:10])
	print("----------------------------------------------------------------------")
	print("Top 10 features based on CFs considering global importance for TPs....")
	print("----------------------------------------------------------------------")
	#print(top_10_features)
	top_10 = list(top_10_features.keys())
	print(top_10)
	
	
	
	return top_10


def find_common_elements(list1, list2):
# Using set intersection to find common elements
	common_elements = set(list1) & set(list2)

	# Counting the number of unique common elements
	unique_count = len(common_elements)

	common_elements_list = list(common_elements)
	

	return unique_count, common_elements_list
	
 
def close_stdout():
    sys.stdout.close()
    
    

#Main Execution

timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
target_column = 'Malware'

is_balanced= True
#is_balanced= False
#print(shap.__version__)
# Two sample lists of strings



if is_balanced:
	redirect_stdout_to_file('results_balanced_' + timestamp + '.txt')

	# Change the folder path accordingly
 
	#input_train_path = '../Resources/permissions/2022/50_50_train_dataset.csv'
	#input_test_path = '../Resources/permissions/2022/90_10_test_dataset.csv'

	input_train_path = '../Resources/permissions-apicalls-xmal/2022/50_50_train_dataset.csv'
	input_test_path = '../Resources/permissions-apicalls-xmal/2022/90_10_test_dataset.csv'

	#input_train_path = '../Resources/permissions-apicalls-mobi/2021/50_50_train_dataset.csv'
	#input_test_path = '../Resources/permissions-apicalls-mobi/2021/90_10_test_dataset.csv'
	
	print(f'Input train path: {input_train_path}')
	print(f'Input test path: {input_test_path}')
	
	train_features_df = pd.read_csv(input_train_path)
	test_features_df = pd.read_csv(input_test_path)
	
	print(f'Train set size: {train_features_df.shape}.')
	print(f'Test set size: {test_features_df.shape}.')
	
	X_train, y_train, X_test, y_test, test_sha256, df_train = prepare_data_for_classification_balanced(train_features_df, test_features_df)
	
	


else:
	redirect_stdout_to_file('results_imbalanced_' + timestamp + '.txt')

	# Change the folder path accordingly
	#dataset_path = '../Resources/permissions/2022/90_10_main_dataset.csv'
	dataset_path = '../Resources/permissions-apicalls-xmal/2022/90_10_main_dataset.csv'
	#dataset_path = '../Resources/permissions-apicalls-mobi/2021/90_10_main_dataset.csv'

	df = load_combined_dataset(dataset_path)
	print(f'Dataset path: {dataset_path}')
	
	malware_column = df['Malware']
	ratio_ones_to_zeroes = malware_column.value_counts(normalize=True)
	
	print("Ratio of Benign APKs to Malware APKs:")
	print(ratio_ones_to_zeroes)

	#X_train, y_train, X_test, y_test, test_sha256, t_train = prepare_data_for_classification_imbalanced(df, 'Malware')
	X_train, y_train, X_test, y_test, test_sha256, df_train = prepare_data_for_classification_imbalanced(df, 'Malware')
	#print(y_test[0])
	
	
	


build_and_evaluate_model()

print("--------------------")
print(f'DateTime: {timestamp}.')
print("--------------------")
close_stdout()





