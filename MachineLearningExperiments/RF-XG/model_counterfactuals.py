from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score,roc_auc_score, confusion_matrix, precision_recall_curve
from sklearn.model_selection import GridSearchCV
import pandas as pd
from xgboost import XGBClassifier
import sys

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest, chi2
import datetime

import dice_ml
from dice_ml.utils import helpers  # helper functions

from sklearn.compose import ColumnTransformer

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import random
from sklearn.inspection import permutation_importance
import numpy as np





def redirect_stdout_to_file(file_path):
    sys.stdout = open(file_path, 'w')
    
def remove_constant_features(df1, df2):
	#	 Identify constant categorical features
	constant_categorical_features1 = [col for col in df1.columns if df1[col].nunique() == 1 and df1[col].dtype == 'object']
	constant_categorical_features2 = [col for col in df2.columns if df2[col].nunique() == 1 and df2[col].dtype == 'object']
	if constant_categorical_features1 == constant_categorical_features2:
		
		print(f"There are {len(constant_categorical_features1)} constant categorical features and we remove them!")
		print(f"Removed features are: {constant_categorical_features1}.")

		# Drop constant categorical features from the DataFrame
		df1 = df1.drop(columns=constant_categorical_features1)
		# Drop constant categorical features from the DataFrame
		df2 = df2.drop(columns=constant_categorical_features2)
		
	return df1, df2


def prepare_data_for_classification(training_data, test_data):


    train = pd.DataFrame(training_data)
    test = pd.DataFrame(test_data)
    # Randomly shuffling the rows of the dataframes
    train = train.sample(frac=1, random_state=42)  # Setting a random seed for reproducibility
    test = test.sample(frac=1, random_state=42)  # Setting a random seed for reproducibility
    train.reset_index(drop=True, inplace=True)
    test.reset_index(drop=True, inplace=True)

    # Calculating the ratio of benign to malware APKs in training set
    malware_column_train = train['Malware']
    ratio_ones_to_zeroes = malware_column_train.value_counts(normalize=True)
    print("TRAINING SET: Ratio of Benign APKs to Malware APKs:")
    print(ratio_ones_to_zeroes)

    # Calculating the ratio of benign to malware APKs
    malware_column_test = test['Malware']
    ratio_ones_to_zeroes = malware_column_test.value_counts(normalize=True)
    print("TEST SET: Ratio of Benign APKs to Malware APKs:")
    print(ratio_ones_to_zeroes)

    
    test_sha256_list = test['sha256']
    
    # Set display options
    pd.set_option('display.max_rows', None)
    pd.set_option('display.max_columns', None)
    train = train.drop(['sha256'], axis=1)
    test = test.drop(['sha256'], axis=1)

    # Print the names of all columns
    column_names = train.columns
    column_names_list = column_names.to_list()

    for col in column_names_list:
        train[col] = train[col].astype(int)
        test[col] = test[col].astype(int)

    # Split data into features and target
    X_train = train.drop(columns=['Malware'])
    y_train = train['Malware']

    # Split data into features and target
    X_test = test.drop(columns=['Malware'])
    y_test = test['Malware']
    
    new_X_train , new_X_test = remove_constant_features(X_train, X_test)


    
    return new_X_train, y_train, new_X_test, y_test, test_sha256_list




def build_and_evaluate_model():


	# Parameter Grid for Random Forest
	

	param_grid_rf = {
		'n_estimators': [50, 100, 200, 500, 1000],
		'max_depth': [None, 10, 16, 20, 30, 40],
		'min_samples_split': [2, 5, 10, 20, 50]
	}
	


	rf_model = RandomForestClassifier()

	grid_search_rf = GridSearchCV(rf_model, param_grid_rf, cv=5, scoring='f1', verbose=2, n_jobs=-1)

	# Fit the models
	grid_search_rf.fit(X_train, y_train)


	# Predict and evaluate using the best model
	best_rf = grid_search_rf.best_estimator_
	
	
	#probabilities = best_rf.predict_proba(X_test)[:,1]
	
	# Calculate precision, recalls, and thresholds
	#precisions, recalls, thresholds = precision_recall_curve(y_test, probabilities)

	# Calculate F1 scores for each threshold
	#f1_scores = 2 * (precisions * recalls) / (precisions + recalls)
	# Handling the case where precision and recall are both zero (could lead to NaN F1 scores)
	#f1_scores = np.nan_to_num(f1_scores)

	# Find the index of the maximum F1 score
	#optimal_idx = np.argmax(f1_scores)
	# Find the optimal threshold corresponding to this F1 score
	#optimal_threshold = thresholds[optimal_idx]

	#print(f"Optimal Threshold: {optimal_threshold}")
	#print(f"Maximum F1 Score: {f1_scores[optimal_idx]}")

	# Apply the optimal threshold to determine class predictions
	#optimal_predictions = (probabilities >= optimal_threshold).astype(int)

	
	#final_precision = precision_score(y_test, optimal_predictions)
	#final_recall = recall_score(y_test, optimal_predictions)

	#print(f"Optimal Threshold: {optimal_threshold}")
	#print(f"Maximum F1 Score: {f1_scores[optimal_idx]}")
	#print(f"Final Precision at Optimal F1 Score: {final_precision}")
	#print(f"Final Recall at Optimal F1 Score: {final_recall}")


	
	




	print("================RANDOM FOREST RESULTS===================:")
	rf_test_predictions = best_rf.predict(X_test)
	rf_train_predictions = best_rf.predict(X_train)

	print("Best Parameters: ", grid_search_rf.best_params_)

	train_accuracy = accuracy_score(y_train, rf_train_predictions)
	test_accuracy = accuracy_score(y_test, rf_test_predictions)

	train_precision = precision_score(y_train, rf_train_predictions)
	test_precision = precision_score(y_test, rf_test_predictions)

	train_recall = recall_score(y_train, rf_train_predictions)
	test_recall = recall_score(y_test, rf_test_predictions)

	train_f1 = f1_score(y_train, rf_train_predictions)
	test_f1 = f1_score(y_test, rf_test_predictions)

	train_probs = best_rf.predict_proba(X_train)[:, 1]  
	test_probs = best_rf.predict_proba(X_test)[:, 1]

	train_roc_auc = roc_auc_score(y_train, train_probs)
	test_roc_auc = roc_auc_score(y_test, test_probs)

	train_conf_matrix = confusion_matrix(y_train, rf_train_predictions)
	test_conf_matrix = confusion_matrix(y_test, rf_test_predictions)

	print(f"Training Accuracy: {train_accuracy}, Test Accuracy: {test_accuracy}")
	print(f"Training Precision: {train_precision}, Test Precision: {test_precision}")
	print(f"Training Recall: {train_recall}, Test Recall: {test_recall}")
	print(f"Training F1 Score: {train_f1}, Test F1 Score: {test_f1}")
	print(f"Training ROC-AUC: {train_roc_auc}, Test ROC-AUC: {test_roc_auc}")
	print(f"Training Confusion Matrix:\n{train_conf_matrix}")
	print(f"Test Confusion Matrix:\n{test_conf_matrix}")

	#------------------------------------
	print(classification_report(y_test, rf_test_predictions, labels=[0, 1], target_names=["Benign", "Malware"]))
	print(classification_report(y_train, rf_train_predictions, labels=[0, 1], target_names=["Benign", "Malware"]))

	# Create a test results DataFrame

	test_results_rf = pd.DataFrame({'SHA256': test_sha256, 'Actual Malware': y_test, 'Predicted label':rf_test_predictions})

	test_results_rf = pd.concat([test_results_rf, X_test], axis=1)

	test_results_rf.to_csv('test_dataset_predictions_rf.csv', index=False)
	
	fp_indices = [i for i in range(len(y_test)) if y_test[i] == 0 and rf_test_predictions[i] == 1]
	fn_indices = [i for i in range(len(y_test)) if y_test[i] == 1 and rf_test_predictions[i] == 0]
	tn_indices = [i for i in range(len(y_test)) if y_test[i] == 0 and rf_test_predictions[i] == 0]
	tp_indices = [i for i in range(len(y_test)) if y_test[i] == 1 and rf_test_predictions[i] == 1]

	# Print the indices for each category

	print(f"Indices of False Positives (FP): {fp_indices}")
	print(f"Number of False Positives (FP): {len(fp_indices)}")
	
	print(f"Indices of False Negatives (FN): {fn_indices}")
	print(f"Number of False Negatives (FN): {len(fn_indices)}")
	
	print(f"Indices of True Negatives (TN): {tn_indices}")
	print(f"Number of True Negatives (TN): {len(tn_indices)}")
	
	print(f"Indices of True Positives (TP): {tp_indices}")
	print(f"Number of True Positives (TP): {len(tp_indices)}")
	
	
	
	false_negative_indices_new  = []  #stores a subset of TPs in which atleast one feature is equal to one
	
	for idx in range(len(fn_indices)):
		# Choose one of the true positives (e.g., the first one)
		
		chosen_index = fn_indices[idx]

		# Get the feature values for the chosen true positive instance
		chosen_instance_features = X_test.iloc[chosen_index]

		
		for column_name, value in chosen_instance_features.items():
			if value == 1:
				#print(column_name)
				if chosen_index not in false_negative_indices_new:
					false_negative_indices_new.append(chosen_index)
					

	# Print the indexes of false negatives
	print(f"Filtered FN indices: {false_negative_indices_new }")
	print(f"Number of filtered FNs: {len(false_negative_indices_new) }")
	
	false_positive_indices_new  = []  #stores a subset of TPs in which atleast one feature is equal to one
	
	
	for idx in range(len(fp_indices)):
		# Choose one of the true positives (e.g., the first one)
		
		chosen_index = fp_indices[idx]

		# Get the feature values for the chosen true positive instance
		chosen_instance_features = X_test.iloc[chosen_index]

		
		for column_name, value in chosen_instance_features.items():
			if value == 1:
				#print(column_name)
				if chosen_index not in false_positive_indices_new:
					false_positive_indices_new.append(chosen_index)
					

	# Print the indexes of false negatives
	print(f"Filtered FP indices: {false_positive_indices_new}")
	print(f"Number of filtered FPs: {len(false_positive_indices_new) }")
	
	print("--------------------Permutation importance for RF ------------------------")
	# Calculate recall as the baseline metric
	baseline_recall = recall_score(y_test, rf_test_predictions)
	print(f"Baseline Recall: {baseline_recall:.2f}")

	# Calculate permutation importance
	perm_importance = permutation_importance(best_rf, X_test, y_test, scoring='recall', n_repeats=30, random_state=42,n_jobs=-1)

	# Get feature importances and sort them
	feature_importances = perm_importance.importances_mean
	# Sort the dictionary based on values (descending order)

	#sorted_importance_descending = dict(sorted(feature_importances.items(), key=lambda item: item[1], reverse=True))
	#print(sorted_dict_descending)
	#for i, v in enumerate(sorted_importance_descending):
		#if v > 0:
			#print(f'Feature: {X_test.columns[i]}, Score: {v}')

	temp_dict ={}
	print(type(feature_importances))
	for i, v in enumerate(feature_importances):
		if v > 0:
			#print(f'Feature: {X_test.columns[i]}, Score: {v}')
			temp_dict[X_test.columns[i]] = v
	sorted_importance_descending = sorted(temp_dict.items(), key=lambda item: item[1], reverse=True)
	#print(sorted_importance_descending)
	k = 20
	top_k_items = sorted_importance_descending[:k]

	# Print the top 10 items
	for item, value in top_k_items:
		print(f'{item}: {value}')
	

	
	return best_rf, false_negative_indices_new , false_positive_indices_new


 

def feature_selection_and_dataframe(input_train_path, input_test_path, target_column, k_features=10):
    # Load your dataset
    train_dataset = pd.read_csv(input_train_path)
    test_dataset = pd.read_csv(input_test_path)

    # Drop the 'sha256' and target columns for feature selection
    df_train_features = train_dataset

    # Assume 'target' is the column you want to predict
    X_train = df_train_features.drop(columns=['sha256', target_column])
    y_train = df_train_features[target_column]
    
    
    X_train_APICalls = df_train_features.drop(df_train_features.iloc[:, 0:350], axis=1)
    print(f'Size of data frame that contains API Calls is {X_train_APICalls.shape}.')
    print(f'The original number of API Calls is {X_train_APICalls.shape[1]}.')
    
    
    X_train_manifest_properties = df_train_features.drop(df_train_features.iloc[:, 350:], axis=1)
    selected_feature_manifest = list(X_train_manifest_properties.columns.values)
    print(f'Number of manifest properties is {len(selected_feature_manifest)}.')
    
    


    # Perform feature selection using chi-squared test
    selector = SelectKBest(chi2, k=k_features)
    X_train_selected = selector.fit_transform(X_train_APICalls, y_train)
    print(f'Number of selected API Calls is {k_features}.')

    # Get the indices of the selected features
    selected_indices = selector.get_support(indices=True)

    # Get the names of the selected features
    selected_feature_names_api_calls = X_train_APICalls.columns[selected_indices]

    # 'sha256' and 'malware' columns are already included in the selected_feature_manifest
    selected_feature_names = list(selected_feature_manifest)+list(selected_feature_names_api_calls)
    print(f' The total number of selected features is {len(selected_feature_names) - 2}.') # We deduct 2 because the features include sha256 and Malware columns
    print(f'\n Selected features: {selected_feature_names}.')

    # Create a new DataFrame with the selected features
    df_selected_train_features = train_dataset[selected_feature_names]
    df_selected_test_features = test_dataset[selected_feature_names]

    return df_selected_train_features, df_selected_test_features
def close_stdout():
    sys.stdout.close()
    
    
def create_counterfactuals(ml_model, df_train, false_negatives_idx):
	print("Creating Counterfactuals....")
	# Setup DiCE
	d_train = dice_ml.Data(dataframe=df_train, continuous_features=[], outcome_name='Malware')


	dice_model=  dice_ml.Model(model=ml_model, backend='sklearn')

	explainer = dice_ml.Dice(d_train, dice_model, method="random")
	# Setting parameters for counterfactual generation
	cf_parameters = {
		"total_CFs": 2,  # Number of counterfactuals to generate
		"desired_class": "opposite",  # Asking for the opposite class
		"proximity_weight": 0.5,  # Weight for proximity loss
		"diversity_weight": 1.0  # Weight for diversity
		
	}

		
	#random_fn = random.choice(false_negatives_idx)

	
	for fn_idx in false_negatives_idx:
		row=X_test.iloc[fn_idx]
		print(f'Malware: {y_test.iloc[fn_idx]}')
		for column_name, value in row.items():
			if value == 1:
				print(f"Column Name: {column_name}")
						
		# Sample query instance from the test set
		query_instance = pd.DataFrame(X_test.iloc[fn_idx]).T
	
		print("Generating Counterfactuals....")

		# Generate counterfactuals
		counterfactuals = explainer.generate_counterfactuals(query_instance, **cf_parameters)

		df_train= df_train.drop(['Malware'], axis=1)

		# Get the original instance as a Pandas DataFrame
		original_instance = pd.DataFrame(query_instance, columns=df_train.columns)

		print("Counterfactual Explanations:")
		#Iterate through the counterfactuals
		for i, cf in enumerate(explainer.final_cfs):
			print(f"Counterfactual {i+1} (Changed Features):")
			
			# Iterate through the features and their values in the counterfactual
			for feature, cf_value, original_value in zip(df_train.columns, cf, original_instance.iloc[0]):
				if cf_value != original_value:
				    print(f"{feature}: Original={original_value}, Counterfactual={cf_value}")

	 
	
			
			
	
	
	#print("Visualizing Counterfactuals....")

	# Print the counterfactuals
	#print(counterfactuals.visualize_as_dataframe(show_only_changes=True))
	print("Local feature importance:")
	imp = explainer.local_feature_importance(query_instance, cf_examples_list=counterfactuals.cf_examples_list)
	print(imp.local_importance)
	
	# Sort the local feature importances by importance scores (descending)
	#sorted_local_feature_importance = dict(sorted(imp.items(), key=lambda item: item[1], reverse=True))

	# Print the top 10 features by local importance score
	#top_10_features = list(sorted_local_feature_importance.keys())[:10]
	#for feature in top_10_features:
		#print(f"Feature: {feature}, Local Importance Score: {sorted_local_feature_importance[feature]}")

	


# Main Execution
timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")



redirect_stdout_to_file('results_balanced_counterfactuals' + timestamp + '.txt')


# Example usage of the function
# input_file_path = '/Users/khushbeen/PycharmProjects/StaticAndroidMalwareAnalysis/MachineLearningModels/FeatureSelection/Resources/qa_dataset.csv'
input_train_path = '../Resources/50_50_train_dataset.csv'
input_test_path = '../Resources/90_10_test_dataset.csv'
target_column = 'Malware'
enable_feature_selection = False


if enable_feature_selection:
	selected_train_features_df, selected_test_features_df = feature_selection_and_dataframe(input_train_path,input_test_path ,target_column)
else: 
	selected_train_features_df = pd.read_csv(input_train_path)
	selected_test_features_df = pd.read_csv(input_test_path)
	print(f'Train set size: {selected_train_features_df.shape}.')
	print(f'Test set size: {selected_test_features_df.shape}.')
	
	


X_train, y_train, X_test, y_test, test_sha256 = prepare_data_for_classification(selected_train_features_df, selected_test_features_df)

print(selected_train_features_df.columns.values) # has Mlaware and sh256 columns

df_train = selected_train_features_df.drop(['sha256'], axis=1)
#sys.exit(0)

model, FN_idx, FP_idx = build_and_evaluate_model()
#create_counterfactuals(model, df_train, FN_idx)



print(f'DateTime: {timestamp}.')

close_stdout()






