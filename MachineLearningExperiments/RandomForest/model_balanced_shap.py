from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score,roc_auc_score, confusion_matrix, precision_recall_curve
from sklearn.model_selection import GridSearchCV
import pandas as pd
from xgboost import XGBClassifier
import sys

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest, chi2
import datetime


from sklearn.compose import ColumnTransformer

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import random
from sklearn.inspection import permutation_importance
import numpy as np
from operator import itemgetter

import shap

import matplotlib.pyplot as plt



def redirect_stdout_to_file(file_path):
    sys.stdout = open(file_path, 'w')
    



def prepare_data_for_classification(training_data, test_data):


    train = pd.DataFrame(training_data)
    test = pd.DataFrame(test_data)
    # Randomly shuffling the rows of the dataframes
    train = train.sample(frac=1, random_state=42)  # Setting a random seed for reproducibility
    test = test.sample(frac=1, random_state=42)  # Setting a random seed for reproducibility
    train.reset_index(drop=True, inplace=True)
    test.reset_index(drop=True, inplace=True)

    # Calculating the ratio of benign to malware APKs in training set
    malware_column_train = train['Malware']
    ratio_ones_to_zeroes = malware_column_train.value_counts(normalize=True)
    print("TRAINING SET: Ratio of Benign APKs to Malware APKs:")
    print(ratio_ones_to_zeroes)

    # Calculating the ratio of benign to malware APKs
    malware_column_test = test['Malware']
    ratio_ones_to_zeroes = malware_column_test.value_counts(normalize=True)
    print("TEST SET: Ratio of Benign APKs to Malware APKs:")
    print(ratio_ones_to_zeroes)

    
    test_sha256_list = test['sha256']
    
    # Set display options
    pd.set_option('display.max_rows', None)
    pd.set_option('display.max_columns', None)
    train = train.drop(['sha256'], axis=1)
    test = test.drop(['sha256'], axis=1)

    # Print the names of all columns
    column_names = train.columns
    column_names_list = column_names.to_list()

    for col in column_names_list:
        train[col] = train[col].astype(int)
        test[col] = test[col].astype(int)

    d_train = train.copy() #train without sha256 col
    # Split data into features and target
    X_train = train.drop(columns=['Malware'])
    y_train = train['Malware']

    # Split data into features and target
    X_test = test.drop(columns=['Malware'])
    y_test = test['Malware']
    
   
       
    return X_train, y_train, X_test, y_test, test_sha256_list, d_train


def build_and_evaluate_model():

	# Parameter Grid for Random Forest
	param_grid_rf = {
		'n_estimators': [50, 100, 200, 500, 1000],
		'max_depth': [None, 10, 16, 20, 30, 40],
		'min_samples_split': [2, 5, 10, 20, 50]
	}
	
	
			
	rf_model = RandomForestClassifier()

	grid_search_rf = GridSearchCV(rf_model, param_grid_rf, cv=5, scoring='f1', verbose=2, n_jobs=-1)

	# Fit the models
	grid_search_rf.fit(X_train, y_train)


	# Predict and evaluate using the best model
	best_rf = grid_search_rf.best_estimator_
	print("================RANDOM FOREST RESULTS===================:")
	rf_test_predictions = best_rf.predict(X_test)
	rf_train_predictions = best_rf.predict(X_train)

	print("Best Parameters: ", grid_search_rf.best_params_)

	train_accuracy = accuracy_score(y_train, rf_train_predictions)
	test_accuracy = accuracy_score(y_test, rf_test_predictions)

	train_precision = precision_score(y_train, rf_train_predictions)
	test_precision = precision_score(y_test, rf_test_predictions)

	train_recall = recall_score(y_train, rf_train_predictions)
	test_recall = recall_score(y_test, rf_test_predictions)

	train_f1 = f1_score(y_train, rf_train_predictions)
	test_f1 = f1_score(y_test, rf_test_predictions)

	train_probs = best_rf.predict_proba(X_train)[:, 1]  
	test_probs = best_rf.predict_proba(X_test)[:, 1]

	train_roc_auc = roc_auc_score(y_train, train_probs)
	test_roc_auc = roc_auc_score(y_test, test_probs)

	train_conf_matrix = confusion_matrix(y_train, rf_train_predictions)
	test_conf_matrix = confusion_matrix(y_test, rf_test_predictions)
	#The order in output
	#[[TN, FP],
	#[FN, TP]]
	TN, FP, FN, TP = test_conf_matrix.ravel()

	print(f"Training Accuracy: {train_accuracy}, Test Accuracy: {test_accuracy}")
	print(f"Training Precision: {train_precision}, Test Precision: {test_precision}")
	print(f"Training Recall: {train_recall}, Test Recall: {test_recall}")
	print(f"Training F1 Score: {train_f1}, Test F1 Score: {test_f1}")
	print(f"Training ROC-AUC: {train_roc_auc}, Test ROC-AUC: {test_roc_auc}")
	print(f"Training Confusion Matrix:\n{train_conf_matrix}")
	print(f"Test Confusion Matrix:\n{test_conf_matrix}")
	print("For test dataset:")
	print(f"True Negatives (TN): {TN}")
	print(f"False Positives (FP): {FP}")
	print(f"False Negatives (FN): {FN}")
	print(f"True Positives (TP): {TP}")

	#------------------------------------
	print(classification_report(y_test, rf_test_predictions, labels=[0, 1], target_names=["Benign", "Malware"]))
	
	# Create a test results DataFrame

	test_results_rf = pd.DataFrame({'SHA256': test_sha256, 'Actual Malware': y_test, 'Predicted label':rf_test_predictions})

	#Extract the SHA256 hashes for FPs, FNs, TPs, TNs
	
	fp_hashes = []
	fn_hashes = []
	tn_hashes = []
	tp_hashes = []
	
	for index, row in test_results_rf.iterrows():
		if row['Actual Malware'] == 0 and row['Predicted label'] == 1:
			fp_hashes.append(row['SHA256'])
		elif row['Actual Malware'] == 1 and row['Predicted label'] == 0:	
			fn_hashes.append(row['SHA256'])
		elif row['Actual Malware'] == 0 and row['Predicted label'] == 0:
			tn_hashes.append(row['SHA256'])
		elif row['Actual Malware'] == 1 and row['Predicted label'] == 1:
			tp_hashes.append(row['SHA256'])

	print(f"SHA256 hashes for False Positives (FP): {fp_hashes}")
	print(f"Number of False Positives (FP): {len(fp_hashes)}")
		
	print(f"SHA256 hashes for False Negatives (FN): {fn_hashes}")
	print(f"Number of False Negatives (FN): {len(fn_hashes)}")
	
	#print(f"SHA256 hashes for True Negatives (TN): {tn_hashes}")
	print(f"Number of True Negatives (TN): {len(tn_hashes)}")
	
	#print(f"SHA256 hashes for True Positives (TP): {tp_hashes}")
	print(f"Number of True Positives (TP): {len(tp_hashes)}")
	
	test_results_rf = pd.concat([test_results_rf, X_test], axis=1)

	test_results_rf.to_csv('test_dataset_predictions_rf'+timestamp+'.csv', index=False)
	
		
	
	fp_indices = [i for i in range(len(y_test)) if y_test[i] == 0 and rf_test_predictions[i] == 1]
	fn_indices = [i for i in range(len(y_test)) if y_test[i] == 1 and rf_test_predictions[i] == 0]
	tn_indices = [i for i in range(len(y_test)) if y_test[i] == 0 and rf_test_predictions[i] == 0]
	tp_indices = [i for i in range(len(y_test)) if y_test[i] == 1 and rf_test_predictions[i] == 1]

	
	
	false_negative_indices_new  = []  #stores a subset of FNs in which atleast one feature is equal to one
	
	for idx in range(len(fn_indices)):
		# Choose one of the false negatives (e.g., the first one)
		
		chosen_index = fn_indices[idx]

		# Get the feature values for the chosen false negative instance
		chosen_instance_features = X_test.iloc[chosen_index]

		
		for column_name, value in chosen_instance_features.items():
			if value == 1:
				#print(column_name)
				if chosen_index not in false_negative_indices_new:
					false_negative_indices_new.append(chosen_index)
					

	# Print the indexes of false negatives
	print(f"Filtered FN indices: {false_negative_indices_new }")
	print(f"Number of filtered FNs: {len(false_negative_indices_new) }")
	
	true_positive_indices_new  = []  #stores a subset of FPs in which atleast one feature is equal to one
	
	
	for idx in range(len(tp_indices)):
		# Choose one of the false positives (e.g., the first one)
		
		chosen_index = tp_indices[idx]

		# Get the feature values for the chosen true positive instance
		chosen_instance_features = X_test.iloc[chosen_index]

		
		for column_name, value in chosen_instance_features.items():
			if value == 1:
				#print(column_name)
				if chosen_index not in true_positive_indices_new:
					true_positive_indices_new.append(chosen_index)
					

	# Print the indexes of false negatives
	print(f"Filtered TP indices: {true_positive_indices_new}")
	print(f"Number of filtered TPs: {len(true_positive_indices_new) }")
	
	
	return best_rf, false_negative_indices_new , true_positive_indices_new


 


def close_stdout():
    sys.stdout.close()
    
def compute_shap_values(ml_model,df_train, false_negatives_idx, true_positives_idx):
	# load JS visualization code to notebook
	shap.initjs()
	explainer = shap.TreeExplainer(ml_model)
	shap_values = explainer.shap_values(X_test)
	#shap_values = shap_explainer(X_test[0, :].reshape(1, -1)).values
	print(len(shap_values))
	print(shap_values)
	#print("Variable Importance Plot - Global Interpretation")
	figure = plt.figure()
	#shap.summary_plot(shap_values, X_test, max_display=10) # top 10 features

	#shap_values[1] is used to represent the SHAP values for instances classified as label 1 (malware).
	#Y-axis represents the features ranked by their average absolute SHAP values
	#X-axis represents SHAP values. Positive values for a given feature push the modelâ€™s prediction closer to the label being examined (label=1). In contrast, negative values push towards the opposite class (label=0).
	shap.summary_plot(shap_values[1], X_test, max_display=10)
	
	#als= np.abs(shap_values).mean(0)

	#feature_importance = pd.DataFrame(list(zip(features.columns, sum(vals))), columns=['col_name','feature_importance_vals'])
	#feature_importance.sort_values(by=['feature_importance_vals'], ascending=False,inplace=True)
	#feature_importance.head()
	
	sys.exit(0)

	print("--------------------------------------")
	print("Generating TreeShap values for TPs....")
	print("--------------------------------------")
	for tp_idx in true_positives_idx:
		print("----------------------------------------------------")
		print(f"Generating TreeShap for tp_idx: {tp_idx}....")
		print("----------------------------------------------------")
		
		
		#tree_shap_values = shap_values[tp_idx]
		fasttreeshap.summary_plot(shap_values, X_test, plot_type="bar")
		#features = X_test.iloc[tp_idx]
		
		# Creating a dictionary to hold 'Feature': (SHAP Value, Feature Value)
		shap_dict = {feature: (shap_val, feature_val)   for feature, shap_val, feature_val in zip(X_test.columns, tree_shap_values, features)}

		# Display the dictionary
		for feature, values in shap_dict.items():
			print(f"{feature}: SHAP Value = {values[0]}, Feature Value = {values[1]}")
		
	# Calculate the mean absolute SHAP value for each feature
	#gobal_importance = np.mean(np.abs(shap_values), axis=0)

	# If you have feature names available:
	#feature_names = list(X_test.columns.values)  # example feature names

	# Combine global importance scores with feature names
	#global_feature_importance = dict(zip(feature_names, global_importance))

	# Sort features by their global importance
	#sorted_global_importance = sorted(global_feature_importance.items(), key=lambda x: x[1], reverse=True)

	# Display the sorted global feature importance
	#for feature, importance in sorted_global_importance:
    	#print(f"{feature}: {importance}")
		#
		
		#try:
				
			# Creating a dictionary to hold 'Feature': (SHAP Value, Feature Value)
			#shap_dict = {feature: (shap_val, feature_val) 
           # for feature, shap_val, feature_val in zip(X_test.columns, sample_shap_values, sample_features)}

			# Display the dictionary
			#for feature, values in shap_dict.items():
    			#print(f"{feature}: SHAP Value = {values[0]}, Feature Value = {values[1]}")
			#none_zero_features = {key:val for key, val in temp_dict.items() if val!=0}
			#print(none_zero_features)
			#top_10_features = dict(sorted(none_zero_features.items(), key=itemgetter(1), reverse=True)[:10])
			
			#top_10_features = sorted(none_zero_features, key=none_zero_features.get, reverse=True)[:10]
			#print("--------------------------------------------")
			#print("Top 10 features considering local importance")
			#print("--------------------------------------------")
			#print(top_10_features)

		#except Exception as e:
				#print(f"Error: {e}")
				#print(f'tp_idx: {tp_idx}')		
				#continue
	
	
	
	
	#print(shap_values_auto.shape)
	print(shap_values)
	
	#print(shap_values_auto[0,:])
	#print(X_test.iloc[0,:])
	
	sample_index = 0
	sample_shap_values = shap_values[sample_index]
	sample_features = X_test.iloc[sample_index]

	# Creating a DataFrame to display SHAP values for each feature
	shap_df = pd.DataFrame({
    'Feature': X_test.columns,
    'SHAP Value': sample_shap_values,
    
	})
	
	print(shap_df)
	#shap_interaction_values_auto = shap_explainer(X_test.iloc[:num_samples], interactions = True).values
	#print(shap_interaction_values_auto.shape)
	#print(shap_interaction_values_auto)
	

	return   
	



# Main Execution

timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

redirect_stdout_to_file('results_balanced_counterfactuals_' + timestamp + '.txt')


# Example usage of the function

# Change the folder path accordingly
input_train_path = '../Resources/permissions/2022/50_50_train_dataset.csv'
input_test_path = '../Resources/permissions/2022/90_10_test_dataset.csv'

#input_train_path = '../Resources/permissions-apicalls-xmal/2022/50_50_train_dataset.csv'
#input_test_path = '../Resources/permissions-apicalls-xmal/2022/90_10_test_dataset.csv'

#input_train_path = '../Resources/permissions-apicalls-mobi/2022/50_50_train_dataset.csv'
#input_test_path = '../Resources/permissions-apicalls-mobi/2022/90_10_test_dataset.csv'


print(f'Input train path: {input_train_path}')
print(f'Input test path: {input_test_path}')


target_column = 'Malware'


train_features_df = pd.read_csv(input_train_path)
test_features_df = pd.read_csv(input_test_path)
print(f'Train set size: {train_features_df.shape}.')
print(f'Test set size: {test_features_df.shape}.')

	


X_train, y_train, X_test, y_test, test_sha256, df_train = prepare_data_for_classification(train_features_df, test_features_df)


#df_train = train_features_df.drop(['sha256'], axis=1)



model, FN_idx, TP_idx = build_and_evaluate_model()
print("\n-------------------------------------------------------------")


compute_shap_values(model, df_train,FN_idx, TP_idx)




print("--------------------")
print(f'DateTime: {timestamp}.')
print("--------------------")

close_stdout()






