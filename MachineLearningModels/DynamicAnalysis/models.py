import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
import warnings
from sklearn.utils import resample

warnings.filterwarnings("ignore")


data = pd.read_csv('apk_dataset.csv')
df = pd.DataFrame(data)

# Set display options
pd.set_option('display.max_rows', None)  # or a large number like 5000
pd.set_option('display.max_columns', None)  # or a large number like 50
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', None)  # For wider columns


# Dropping non-numeric columns
df = df.sort_values(by='Timestamp')

df = df.drop(['APK Name', 'SHA Key', 'Timestamp'], axis=1)

object_cols = ['Frequency of System Calls', 'Count of DNS requests', 'Count of outbound connections', 'Domains/IPs contacted', 'Data volume sent', 'Data volume received', 'Number of files created', 'Number of files read', 'Number of files deleted', 'Process spawning', 'Loaded DLLs', 'Intents sent', 'Intents received', 'Checks for root privileges', 'Camera access frequency', 'Microphone access frequency', 'GPS query frequency', 'Frequency of cryptographic API calls', 'Autostart entries created', 'CPU usage patterns', 'Memory usage patterns']

for col in object_cols:
    df[col] = df[col].astype(float)


df = df.fillna(df.mean())

# Split data into features and target
X = df.drop(columns=['Malware Label'])  # Dropping the target and the timestamp columns
y = df['Malware Label'].astype(int)


# Splitting data into training and testing sets
train_size = int(0.9 * len(data))
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Parameter Grid for Random Forest
param_grid_rf = {
    'n_estimators': [50, 100, 200, 500],
    'max_depth': [None, 10, 16, 20, 30],
    'min_samples_split': [2, 5, 10, 20]
}

# Define the parameter grid for XGBoost
param_grid_xgb = {
    'n_estimators': [50, 100, 200, 500],
    'max_depth': [3, 6, 9, 12],
    'learning_rate': [0.001, 0.01, 0.1, 0.3],
    'subsample': [0.5, 0.8, 1.0],
    'colsample_bytree': [0.5, 0.8, 1.0]
}

rf_model = RandomForestClassifier()
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')  # Avoiding a warning related to the label

grid_search_rf = GridSearchCV(rf_model, param_grid_rf, cv=5, verbose=2, n_jobs=-1)
grid_search_xgb = GridSearchCV(xgb_model, param_grid_xgb, cv=5, verbose=2, n_jobs=-1)

# Fit the models
grid_search_rf.fit(X_train, y_train)
grid_search_xgb.fit(X_train, y_train)

# Predict and evaluate using the best model
best_rf = grid_search_rf.best_estimator_
best_xgb = grid_search_xgb.best_estimator_

print("================RANDOM FOREST RESULTS===================:")
y_pred = best_rf.predict(X_test)
print("Best Parameters: ", grid_search_rf.best_params_)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Predict and evaluate using the best model

print("================XGB BOOST RESULTS===================:")
y_pred = best_xgb.predict(X_test)
print("Best Parameters: ", grid_search_xgb.best_params_)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))


print("Calculating Bias and Variance")

n_iterations = 100  # Number of bootstrap samples to create
y_pred_all_rf = []
y_pred_all_xgb = []

best_rf = RandomForestClassifier(max_depth=None, min_samples_split=2, n_estimators=50, random_state=42)
best_xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', colsample_bytree=0.5, learning_rate=0.001,
                    max_depth=3, n_estimators=50, subsample=0.5, random_state=42)

for i in range(n_iterations):
    X_resampled, y_resampled = resample(X_train, y_train)

    best_rf.fit(X_resampled, y_resampled)
    y_pred_rf = best_rf.predict(X_test)
    y_pred_all_rf.append(y_pred_rf)

    best_xgb.fit(X_resampled, y_resampled)
    y_pred_xgb = best_xgb.predict(X_test)
    y_pred_all_xgb.append(y_pred_xgb)


# Average predictions across all models for each instance in the test set
avg_preds_rf = np.mean(y_pred_all_rf, axis=0)
avg_preds_xgb = np.mean(y_pred_all_xgb, axis=0)

bias_rf = np.mean(avg_preds_rf - y_test)
variance_rf = np.mean((y_pred_all_rf - avg_preds_rf) ** 2)

bias_xgb = np.mean(avg_preds_xgb - y_test)
variance_xgb = np.mean((y_pred_all_xgb - avg_preds_xgb) ** 2)


print("============RANDOM FOREST RESULTS=============")
print(f"Bias: {bias_rf}")
print(f"Variance: {variance_rf}")

print("============XGBOOST RESULTS=============")
print(f"Bias: {bias_xgb}")
print(f"Variance: {variance_xgb}")


