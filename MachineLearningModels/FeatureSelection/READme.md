# Feature Selection

### Problem at hand
Our dataset consists of 2600 columns and 10,000 rows.
The dataset is being used for detecting if the Android APK is a malware or not.
We have already built Neural Networks (Liquid) which gave sub-par results as we were not able to experiment with hyperparamter tunning due to limited memory.

We are now focuing on doing feature selection to solve the above problem. Having less features (> 100) will enable us in going a wide gri search with hyper parameters and we can improve our results.

### EDA (Exploratory Data Analysis)

We did attempt to do exploratory data analysis for our dataset but due to a big dataset, we only focused on a subset of features.

### Correlation Analysis

1) The 'sha256' column is dropped as it is not relevant for correlation analysis.
2) The correlation matrix is computed using the corr method in pandas.
3) A threshold (correlation_threshold) is set to identify highly correlated features.
4) A mask is created to identify which features have a correlation above the threshold.
5) Diagonal elements are excluded from the mask to avoid self-correlation.
6) Features to be dropped are identified, and highly correlated features are removed from the dataset.

- 0.8 -> Identified 721 features that are highly correlated and can be removed to avoid redundancy.
- 0.9 -> Identified 705 features that are highly correlated and can be removed to avoid redundancy.


### Uni variate Feature Selection

We performed Uni variate feature selection using ChiSquare test.

<b>Chi-squared Test:</b>
Nature of Target Variable: Categorical (binary).
Type of Features: Categorical.
Example Usage: When both the target variable and features are categorical. It is commonly used for feature selection in classification problems with categorical features.

1) The 'sha256' column is dropped as it is not relevant for univariate feature selection.
2) The dataset is split into features (X) and the target variable (y).
3) The SelectKBest class is used with either the chi-squared or ANOVA test.
4) The selected features are transformed using fit_transform.
5) The indices and names of the selected features are displayed.

Todo -> Recursive feature elimination

### Dimensionality Reduction Techniques
